(Final Product: [Demo](https://pointlessprojects.com/somehash/) and [Code](https://github.com/TravisBumgarner/pointless-projects/tree/main/somehash))

I've always been fascinated by perceived time. A website that takes 2 seconds to load has an actual load time of 2 seconds. Adding visual indicators that things are loading, before they've actually loaded, can trick our minds into thinking things are happening more quickly, perceived time. 

Blurhash is one such visual indicator. Blurhash will process a website's images and generate very small pretty placeholders, roughly 99% smaller than the images they represent. Then, when a user visits the site, those placeholders are loaded, first and quickly. 

I've always been fascinated by this inbetween space. More words here. 

I always love pulling back the curtain on crazy technical things so let me take you on a journey of building my own image previewer, somehash, from scratch.

I've always been curious about the creative opportunities one has when designing a website. 

## Overview

The library is broken down into two parts. Step one is to process all the images and generate somehashes. The somehashes are then stored somewhere such as a database. This process is time and resource intensive so we do it outside of the website. Then in step two, the somehashes are served to the user and a custom React component that knows how to render them does so while the full resolution images load in the background. 

## Step 0: Getting Creative With It

This section itself could probably several blog posts. The creative possibilities are endless. There are tons of algorithms for extracting interesting colors, and textures, and patterns, and gradients from images. I feel like in this regard, I'm just scratching the surface. 

There are two questions you need to answer.

First, what information do you want to extract from the image? The blurhash extracts about 20 to 30 characters or about 20 bytes of data per image. To keep with the performance that blurhash offers, we'll set this as a constraint. 

For the proof of concept, I went with [KMeans clustering](https://medium.com/analytics-vidhya/color-separation-in-an-image-using-kmeans-clustering-using-python-f994fa398454). The short summary of KMeans clustering is that it can be used to extract dominant colors from an image. In the screenshot below you can see the colors extracted from the image. 

![Screenshot 2025-01-03 at 5.30.44 PM.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/32b62e89-af52-43b8-8863-88fa1b34d69c/07d383fb-42c9-4bc3-89d1-a50b4e85e5ef/Screenshot_2025-01-03_at_5.30.44_PM.png)

The second question, is how do we want to display the extracted data? Our constraint here is that whatever we choose should do its thing (load and optionally animate) before the image loads. At our disposal, we have the entirety of JavaScript and CSS to do what we want. 

## Step 1: Extract Data

The first step is to process the images that'll be displayed on the website. Image processing is an intensive task. It can take roughly several seconds to process an image and extract information. Because of this, a script that runs outside of the browser is selected. 

**Selecting Tools**

The language of choice for this task will be Python. Python has an amazing collection of libraries such as [Pillow](https://pypi.org/project/pillow/), [NumPy](https://numpy.org/), and [OpenCV](https://opencv.org/) for reading and analyzing images.

**Extracting Data**

As mentioned above, I went with KMeans clustering. 

```python
def kmeans_colors(image, num_colors=5):   
    kmeans = KMeans(n_clusters=num_colors)
    kmeans.fit(image)
    colors = kmeans.cluster_centers_.astype(int)
    
    return colors, aspect_ratio
```

**Encoding Data**

The next step is to get the information from the Python script to React. I'll be honest, I know almost nothing about compression algorithms. If I were actually setting this up for production, this would probably be worth a few days of tinkering and research but it was not in scope for this project. I did add a bit of future proofing with a `version` field so that if I were to export multiple visualizations, my React component would know how to handle it. 

One other import bit that I learned in hindsight is that dealing with somehashes (and blurhashes) where you don't know the aspect ratio is a pain. [Cumulative layout shift](https://web.dev/articles/cls) is a thing to consider. 

```python
def encode_animated_lines(colors, aspect_ratio):
    color_string = '_'.join([f'{r}-{g}-{b}' for r, g, b in colors])
    
    encoded_bytes = base64.b64encode(color_string.encode('utf-8'))
    encoded_string = encoded_bytes.decode('utf-8')
    
    return {'version': 'animated_lines', 'hash': encoded_string, 'aspect_ratio': aspect_ratio}
```

## Step 2: Render Previews

### **Decode Data in React**

This step is the opposite of the previous step. The data stored in the JSON file is decoded and passed to the component responsible for rendering the specific `version` of the  hash. 

### Render with Custom React Component

[video]

## Some Area of Improvement

The React component currently assumes a sufficiently fast internet that when the animation finishes loading, the image will have already been loaded. If the Chrome dev tool toggle for “3G” internet is toggled on, you'll observe that the screen is white after the animation finishes. One improvement here is to keep the animation going until the `img` tag's `onload` event fires and then trigger a quick animation to display the image. 

Outside of the scope of this exploration into loading, performance wasn't considered. I'd say there is some area of improvement like not rendering the vertical lines after the image has loaded. But I'll leave that as an exercise to the reader. 

## Fin