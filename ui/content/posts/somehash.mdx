import Figure from '../../app/_sharedComponents/Figure'
import Video from '../../app/_sharedComponents/Video'

(Final Product: [Demo](https://pointlessprojects.com/somehash/) and [Code](https://github.com/TravisBumgarner/pointless-projects/tree/main/somehash))

A website takes five seconds to load. What occurs during those five seconds can impact how a user perceives the experience. Offer them nothing but a wall of white and they might get bored and leave. Capture their attention and they'll stick around and feel like the website loads faster?

How can this be achieved?

[Blurhash](https://blurha.sh/) is one such method. This library generates incredibly small preview images, roughly 99.99% smaller than the full resolution images. They load almost immediately, are visually appealing, and do a great job of capturing the user's attention.

I love this in-between space. It's a space often neglected and de-prioritized. I think it's a space not often explored. Today, I want to explore this space, pull back the curtain, and build my own image previewer, somehash, from scratch.

Let's go!

## Overview

The journey of an image is broken up into three parts - processing, previewing, and loading. First, information is extracted from the image put into a hash and stored. Second, a React component extracts the hash and displays a preview. Finally, the browser loads the image. 

## Step 0: Creative Exploration

The creative possibilities are endless. There are tons of algorithms for extracting interesting colors, textures, patterns, gradients, and more from images. 

There are two questions I needed to answer in my creative exploration.

**What information do I want to extract from the image?**

Blurhash extracts about 20 to 30 characters or about 20 bytes of data per image. This will be my performance benchmark. 

For the proof of concept, I went with [KMeans clustering](https://medium.com/analytics-vidhya/color-separation-in-an-image-using-kmeans-clustering-using-python-f994fa398454). This algorithm is used to extract dominant colors from an image. In the screenshot below you can see an example.

<Figure src="/post-resources/somehash/colors.png" caption="Dominant colors extracted from a photo" />

**How do I want to display the extracted data?**

Whatever effect that is chosen should occur quickly and work on both fast and slow internet connections.

<Video src="/post-resources/somehash/preview.mp4" aspectRatio='1564/1184' />

## Step 1: Extract Data

The first step is to process the images that'll be displayed on the website. Image processing is an intensive task. It can take roughly several seconds to process an image and extract information. Because of this, a script that runs outside of the browser is selected. 

**Selecting Tools**

The language of choice for this task will be Python. Python has an amazing collection of libraries such as [Pillow](https://pypi.org/project/pillow/), [NumPy](https://numpy.org/), and [OpenCV](https://opencv.org/) for reading and analyzing images.

**Extracting Data**

As mentioned above, I went with KMeans clustering. 

```python
def kmeans_colors(image, num_colors=5):   
    kmeans = KMeans(n_clusters=num_colors)
    kmeans.fit(image)
    colors = kmeans.cluster_centers_.astype(int)
    
    return colors, aspect_ratio
```

**Encoding Data**

The next step is to get the information from the Python script to React. I'll be honest, I know almost nothing about compression algorithms. If I were actually setting this up for production, this would probably be worth a few days of tinkering and research but it was not in scope for this project. I did add a bit of future proofing with a `version` field so that if I were to export multiple visualizations, my React component would know how to handle it. 

One other import bit that I learned in hindsight is that dealing with somehashes (and blurhashes) where you don't know the aspect ratio is a pain. [Cumulative layout shift](https://web.dev/articles/cls) is a thing to consider. 

```python
def encode_animated_lines(colors, aspect_ratio):
    color_string = '_'.join([f'{r}-{g}-{b}' for r, g, b in colors])
    
    encoded_bytes = base64.b64encode(color_string.encode('utf-8'))
    encoded_string = encoded_bytes.decode('utf-8')
    
    return {'version': 'animated_lines', 'hash': encoded_string, 'aspect_ratio': aspect_ratio}
```

## Step 2: Render Previews

### **Decode Data in React**

This step is the opposite of the previous step. The data stored in the JSON file is decoded and passed to the component responsible for rendering the specific `version` of the  hash. 

### Render with Custom React Component

[video]

## Some Area of Improvement

The React component currently assumes a sufficiently fast internet that when the animation finishes loading, the image will have already been loaded. If the Chrome dev tool toggle for “3G” internet is toggled on, you'll observe that the screen is white after the animation finishes. One improvement here is to keep the animation going until the `img` tag's `onload` event fires and then trigger a quick animation to display the image. 

Outside of the scope of this exploration into loading, performance wasn't considered. I'd say there is some area of improvement like not rendering the vertical lines after the image has loaded. But I'll leave that as an exercise to the reader. 

## Fin